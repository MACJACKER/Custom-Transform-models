# -*- coding: utf-8 -*-
"""a2_p2_Borad_115854642.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ePvzxbNb0aRXqA1-eX1jDDdPNJLRGGib

Checkpoint 2.1
"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

if torch.cuda.is_available():
    device = torch.device("cuda")

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = AutoModelForCausalLM.from_pretrained("distilgpt2")

# Load BoolQ validation dataset
dataset = load_dataset("boolq", split="validation")

def get_prediction(passage, question):
    prompt = f"{passage}\n{question}?"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=300)

    with torch.no_grad():
        logits = model(**inputs).logits

    yes_token_id = tokenizer.encode(" yes", add_special_tokens=False)[0]
    no_token_id = tokenizer.encode(" no", add_special_tokens=False)[0]

    yes_logit = logits[0, -1, yes_token_id]
    no_logit = logits[0, -1, no_token_id]

    return "yes" if yes_logit > no_logit else "no"


predictions = []
labels = []

for item in dataset:
    predicted_answer = get_prediction(item['passage'], item['question'])
    predictions.append(predicted_answer)
    labels.append("yes" if item['answer'] else "no")

# Evaluation
accuracy = accuracy_score(labels, predictions)
precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')
class_scores = precision_recall_fscore_support(labels, predictions, average=None, labels=["yes", "no"])

print(f"Overall: acc: {accuracy:.3f}, f1: {f1:.3f}\n")
print(f"    Yes: prec: {class_scores[0][0]:.3f}, rec: {class_scores[1][0]:.3f}, f1: {class_scores[2][0]:.3f}")
print(f"     No: prec: {class_scores[0][1]:.3f}, rec: {class_scores[1][1]:.3f}, f1: {class_scores[2][1]:.3f}")

import a2_p1_Borad_115854642

"""Checkpoint 2.2 :To determine the zero shot accuracy using the TrigramLM from Checkpoint 1"""

from transformers import AutoTokenizer
from a2_p1_Borad_115854642 import TrigramLM

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained('distilgpt2')

# Add special tokens and adjust the tokenizer's vocabulary
tokenizer.add_special_tokens({'additional_special_tokens': ['<s>', '</s>', 'OOV']})

# Get a list of all tokenizer's vocabulary plus the special tokens
tokenizer_vocab_list = list(tokenizer.get_vocab().keys()) + ['<s>', '</s>', 'OOV']

# Initialize TrigramLM with the complete vocabulary
lm = TrigramLM(tokenizer_vocab_list)

dataset = load_dataset("boolq", split="validation")

def predict_yes_no(lm, passage, question):

    # Tokenize the context (passage + question)
    context_tokens = ['<s>'] + tokenizer.tokenize(f"{passage} {question}") + ['</s>']

    # Computing probabilities for "yes" and "no"
    yes_prob = lm.nextProb(context_tokens, ['yes'])['yes']
    no_prob = lm.nextProb(context_tokens, ['no'])['no']

    # Prediction based on higher probability
    return "yes" if yes_prob > no_prob else "no"

# Preparing dataset and labels for prediction and evaluation
predictions = []
true_labels = ["yes" if item['answer'] else "no" for item in dataset]

# Making predictions
for item in dataset:
    prediction = predict_yes_no(lm, item['passage'], item['question'])
    predictions.append(prediction)

# Evaluation
accuracy = accuracy_score(true_labels, predictions)
precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, labels=['yes', 'no'], average='macro', zero_division=0)

# Class-specific scores
class_precision_recall_f1 = precision_recall_fscore_support(true_labels, predictions, labels=['yes', 'no'], average=None, zero_division=1)

print(f"Overall: acc: {accuracy:.3f}, f1: {f1:.3f}\n")
print(f"    Yes: prec: {class_precision_recall_f1[0][0]:.3f}, rec: {class_precision_recall_f1[1][0]:.3f}, f1: {class_precision_recall_f1[2][0]:.3f}")
print(f"     No: prec: {class_precision_recall_f1[0][1]:.3f}, rec: {class_precision_recall_f1[1][1]:.3f}, f1: {class_precision_recall_f1[2][1]:.3f}")

"""Checkpoint 2.3 - Determining the training loss curve for the fine tune model using adam optimizer"""

from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW
import torch
from datasets import load_dataset
import matplotlib.pyplot as plt

if torch.cuda.is_available():
    device = torch.device("cuda")
# Load the BoolQ validation dataset subset
dataset = load_dataset("boolq", split="validation")
# Tokenizer initialization
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token
# Model initialization
model = AutoModelForCausalLM.from_pretrained("distilgpt2")

if torch.cuda.is_available():
    model.cuda()

# Configure optimizer
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)

# Training loop
epochs = 3
batch_size = 4  # Start with a small batch size
total_steps = len(dataset) // batch_size * epochs
losses = []  # List to store losses

model.train()
for epoch in range(epochs):
    epoch_losses = []  # List to store losses for each epoch
    for i in range(0, len(dataset), batch_size):
        batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
        passages = [example["passage"] for example in batch]
        questions = [example["question"] for example in batch]
        answers = [example["answer"] for example in batch]

        # Tokenize inputs
        input_texts = [f"{p} {q} {'yes' if a else 'no'}" for p, q, a in zip(passages, questions, answers)]
        inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True, max_length=1024)

        # Move tensors to the same device as model
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Forward pass
        outputs = model(**inputs, labels=inputs["input_ids"])

        # Compute loss
        loss = outputs.loss
        epoch_losses.append(loss.item())  # Record loss for this batch

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Print average loss for the epoch
    epoch_loss_avg = sum(epoch_losses) / len(epoch_losses)
    print(f"Epoch [{epoch+1}/{epochs}], Average Loss: {epoch_loss_avg}")

    # Store average loss for the epoch
    losses.append(epoch_loss_avg)

# Plot the loss curve
plt.plot(losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.legend()
plt.show()

"""Checkpoint 2.4 : Determining the zero shot accuracy on the fine tune model"""

from torch.nn.functional import softmax
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

model.eval()  # Set model to evaluation mode

all_predictions = []
all_true_answers = []

with torch.no_grad():
    for i in range(0, len(dataset), batch_size):
        batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
        passages = [example["passage"] for example in batch]
        questions = [example["question"] for example in batch]
        answers = [example["answer"] for example in batch]

        # Prepare inputs and move tensors to the same device as model
        input_texts = [f"{p} {q}" for p, q in zip(passages, questions)]
        inputs = tokenizer(input_texts, return_tensors="pt", padding=True, truncation=True, max_length=1024)
        inputs = {key: value.to(device) for key, value in inputs.items()}

        outputs = model(**inputs)

        logits = outputs.logits[:, -1, :]
        probs = torch.softmax(logits, dim=-1)
        yes_token_id = tokenizer.convert_tokens_to_ids('yes')
        no_token_id = tokenizer.convert_tokens_to_ids('no')
        predicted_answers = probs[:, [yes_token_id, no_token_id]].argmax(dim=1).cpu().numpy()

        all_predictions.extend(predicted_answers)
        all_true_answers.extend([1 if a else 0 for a in answers])

# Converting all true answers and predictions to numpy arrays
all_true_answers = np.array(all_true_answers)
all_predictions = np.array(all_predictions)

#overall metrics
accuracy = accuracy_score(all_true_answers, all_predictions)
precision, recall, f1, _ = precision_recall_fscore_support(all_true_answers, all_predictions, average='binary')
print(f"Overall: acc: {accuracy:.3f}, f1: {f1:.3f}")

#class-specific metrics
precision_class, recall_class, f1_class, _ = precision_recall_fscore_support(all_true_answers, all_predictions, labels=[1, 0])

# 'Yes' class metrics
print(f"\n    Yes: prec: {precision_class[0]:.3f}, rec: {recall_class[0]:.3f}, f1: {f1_class[0]:.3f}")

# 'No' class metrics
print(f"\n     No: prec: {precision_class[1]:.3f}, rec: {recall_class[1]:.3f}, f1: {f1_class[1]:.3f}")
